{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977f10ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from helper_functions import *\n",
    "\n",
    "N_SAMPLES = 10000\n",
    "NUM_CLASSES = 10\n",
    "NUM_FEATURES = 10\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "print(f\"Torch: version {torch.__version__}\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.set_default_device(device)\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4875548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiclass Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd89326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "X_blob, y_blob = make_blobs(n_samples=N_SAMPLES,\n",
    "                            n_features=NUM_FEATURES,\n",
    "                            centers=NUM_CLASSES,\n",
    "                            cluster_std=.2,\n",
    "                            random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf18927",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_blob = torch.from_numpy(X_blob).type(torch.float)\n",
    "y_blob = torch.from_numpy(y_blob).type(torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b6f2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_blob, y_blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab36966",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(X_blob[:, 0], X_blob[:, 1], c = y_blob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541e0386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets build the model\n",
    "class MultiClass(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(2, 4)        \n",
    "\n",
    "    def forward(self, x : torch.tensor):\n",
    "        self.l1(x)\n",
    "\n",
    "model = MultiClass()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e971987e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printClass(X_, y_, n_classes):\n",
    "    y_train_blob = torch.zeros(len(y_))\n",
    "    for i in range(len(y_train_blob)):\n",
    "        for c in range(n_classes):\n",
    "            if int(y_[i][c].item()) == 1:\n",
    "                y_train_blob[i] = c\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.scatter(X_[:, 0], X_[:, 1], c = y_train_blob)\n",
    "\n",
    "# Calculate accuracy - out of 100 what percent the model gets right\n",
    "def accuracy_fn(y_test, preds):\n",
    "    correct = (y_test == preds).type(torch.float)\n",
    "    nc = (torch.count_nonzero(correct))\n",
    "    return  nc.item() / len(y_test) * 100\n",
    "\n",
    "y = torch.zeros(N_SAMPLES, NUM_CLASSES)\n",
    "for i in range(len(y_blob)):\n",
    "    y[i][int(y_blob[i].item())] = 1\n",
    "\n",
    "# Model with non linearity\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(NUM_FEATURES, 8),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(8, 8),\n",
    "    nn.Linear(8, NUM_CLASSES),\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_blob, y, test_size=0.33, random_state=432)\n",
    "X_train, X_test, y_train, y_test\n",
    "\n",
    "printClass(X_train, y_train, NUM_CLASSES)\n",
    "\n",
    "# Training\n",
    "# Loss for classification\n",
    "loss_fn = nn.MultiLabelSoftMarginLoss() # This is also the output activation function; so is combining Sigmoid -> BCELoss \n",
    "# and is more numerically stable\n",
    "\n",
    "optimizer = optim.SGD(\n",
    "    params=model.parameters(),\n",
    "    lr = 0.03\n",
    ")\n",
    "# Training\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "epochs = 5000\n",
    "for ep in range(epochs):\n",
    "    model.train()\n",
    "\n",
    "    # Logits\n",
    "    y_logits = model(X_train).squeeze()\n",
    "    # Using sigmoid activation and rounding\n",
    "    y_pred = torch.round(torch.sigmoid(y_logits))\n",
    "    # loss and accuracy\n",
    "    loss = loss_fn(y_logits, # we expect raw logits\n",
    "                   y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # backpropagation\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        test_logits = model(X_test).squeeze()\n",
    "        test_pred = torch.round(torch.sigmoid(test_logits))\n",
    "\n",
    "        # test loss\n",
    "        test_loss = loss_fn(test_logits, y_test)\n",
    "\n",
    "    if ep % (epochs / 10) == 0:\n",
    "        print(f\"Ep: {ep} | loss: {loss:.5f} | tloss: {test_loss:.5f} \")\n",
    "\n",
    "# Lets visualize\n",
    "with torch.inference_mode():\n",
    "    test_logits = model(X_test).squeeze()\n",
    "    test_pred = torch.round(torch.sigmoid(test_logits))\n",
    "printClass(X_test, test_pred, NUM_CLASSES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053092cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hello"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
